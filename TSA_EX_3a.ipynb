{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWtdBFPSoeIo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import gutenberg\n",
        "from pprint import pprint\n",
        "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
        "sample_text = 'We will discuss briefly about the basic syntax,\\\n",
        "structure and design philosophies. \\\n",
        "There is a defined hierarchical syntax for Python code which you should remember \\ when writing code! Python is a really powerful programming language!' ## default sentence tokenizer\n",
        "default_st = nltk.sent_tokenize\n",
        "alice_sentences = default_st(text=alice)\n",
        "sample_sentences = default_st(text=sample_text)\n",
        "print 'Total sentences in sample_text:', len(sample_sentences)\n",
        "print 'Sample text sentences :-'\n",
        "pprint (sample_sentences)\n",
        "print '\\nTotal sentences in alice:', len(alice_sentences)\n",
        "print 'First 5 sentences in alice:-'\n",
        "pprint(alice_sentences[0:5])\n",
        "\n",
        "\n",
        "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
        "\n",
        "\n",
        "default_wt = nltk.word_tokenize\n",
        "words = default_wt(sentence)\n",
        "print words\n",
        "\n",
        "\n",
        "treebank_wt = nltk.TreebankWordTokenizer()\n",
        "words = treebank_wt.tokenize(sentence)\n",
        "print words\n",
        "\n",
        "\n",
        "TOKEN_PATTERN = r'\\w+'\n",
        "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False) words = regex_wt.tokenize(sentence)\n",
        "print (words)\n",
        "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
        "print (tagged_sent)\n",
        "\n",
        "\n",
        "from nltk.corpus import treebank\n",
        "nltk.download('treebank')\n",
        "data = treebank.tagged_sents()\n",
        "train_data = data[:3500]\n",
        "test_data = data[3500:]\n",
        "print(train_data[0])\n",
        "\n",
        "\n",
        "from nltk.tag import DefaultTagger\n",
        "dt = DefaultTagger('NN')\n",
        "print(dt.accuracy(test_data))\n",
        "print(dt.tag(tokens))\n"
      ]
    }
  ]
}