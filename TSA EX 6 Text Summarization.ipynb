{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcau0NTKhpr-",
        "outputId": "20cdd779-9369-4e69-feeb-b9c7bcfd55af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WwU2VOe5jzYD",
        "outputId": "65c268de-a3ae-4178-915c-9011afa3deb2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (1.0.0)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.5/90.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.13.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.10.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.6)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.5.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.8.30)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.17.1)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.7.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-4.0.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (75.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2.8.2)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (1.16.0)\n",
            "Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Downloading CherryPy-18.10.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.8/349.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading jaraco.collections-5.1.0-py3-none-any.whl (11 kB)\n",
            "Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Downloading tempora-5.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
            "Downloading jaraco.text-4.0.0-py3-none-any.whl (11 kB)\n",
            "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
            "Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=ca7c1a41351280770841346fd3d6e15631788994b61cbee8f660b07027b68bee\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.5-cp310-cp310-linux_x86_64.whl size=124773 sha256=08b36d8a6845406f208efe4a91cc0cfabfd0bcda2a579b6169fb5a2672699579\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/91/c6/2d9a9e3b935a658c8bfbe2e4eadd1540ecf0d802553195ddc6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=6a0f13de7777f79a06a02a90e921056ccd3d0cbdfefd1566d56fb542fca9d026\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.10.0 feedparser-6.0.11 jaraco.collections-5.1.0 jaraco.context-6.0.1 jaraco.functools-4.1.0 jaraco.text-4.0.0 mysqlclient-2.2.5 pattern-3.6 pdfminer.six-20240706 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.7.0 zc.lockfile-3.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jaraco"
                ]
              },
              "id": "8ad6739dcc6b40c69e4c04079c0376cf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "def parse_document(document):\n",
        "  document = re.sub('\\n', ' ', document)\n",
        "  if isinstance(document, str):\n",
        "    document = document\n",
        "  elif isinstance(document, unicode):\n",
        "    return unicodedata.normalize('NFKD', document).encode('ascii','ignore')\n",
        "  else:\n",
        "    raise ValueError('Document is not string or unicode!')\n",
        "  document = document.strip()\n",
        "  sentences = nltk.sent_tokenize(document)\n",
        "  sentences = [sentence.strip() for sentence in sentences]\n",
        "  return sentences\n",
        "\n",
        "document = '''Café à la mode\n",
        "            This is cat\n",
        "            They eat apple'''\n",
        "print(parse_document(document))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXOzINp9hU7G",
        "outputId": "5a4c2122-ace6-46a3-e74b-b4c43e60fdb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Café à la mode             This is cat             They eat apple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unescaping HTML Sequence\n",
        "import html\n",
        "def unescape_html(text):\n",
        "    return html.unescape(text)\n",
        "print(unescape_html(\"&lt;html&gt;hai&lt;/html&gt;\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdSvhXaejSR9",
        "outputId": "741763a8-c0b4-4a1c-d477-c78dae19213a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html>hai</html>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contractions import CONTRACTION_MAP\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_text(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  return tokens\n",
        "\n",
        "def expand_contractions(text, contraction_mapping):\n",
        "  contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
        "  flags=re.IGNORECASE|re.DOTALL)\n",
        "  def expand_match(contraction):\n",
        "    match = contraction.group(0)\n",
        "    first_char = match[0]\n",
        "    expanded_contraction = contraction_mapping.get(match)\\\n",
        "    if contraction_mapping.get(match)\\\n",
        "    else contraction_mapping.get(match.lower())\n",
        "    expanded_contraction = first_char+expanded_contraction[1:]\n",
        "    return expanded_contraction\n",
        "  expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "  expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "  return expanded_text\n",
        "\n",
        "from pattern.en import tag\n",
        "from nltk.corpus import wordnet as wn\n",

        "def pos_tag_text(text):\n",

        "  def penn_to_wn_tags(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "      return wn.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "      return wn.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "      return wn.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "      return wn.ADV\n",
        "    else:\n",
        "      return None\n",
        "  tagged_text = tag(text)\n",
        "  tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
        "  for word, pos_tag in\n",
        "  tagged_text]\n",
        "  return tagged_lower_text\n",
        "\n",
        "\n",

        "def lemmatize_text(text):\n",
        "  pos_tagged_text = pos_tag_text(text)\n",
        "  lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
        "  else word\n",
        "  for word, pos_tag in pos_tagged_text]\n",
        "  lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "  return lemmatized_text\n",
        "\n",

        "def remove_special_characters(text):\n",
        "  tokens = tokenize_text(text)\n",
        "  pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
        "  filtered_tokens = filter(None, [pattern.sub('', token) for token in\n",
        "  tokens])\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "  return filtered_text\n",
        "\n",

        "def remove_stopwords(text):\n",
        "  tokens = tokenize_text(text)\n",
        "  filtered_tokens = [token for token in tokens if token not in\n",
        "  stopword_list]\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "  return filtered_text\n"
      ],
      "metadata": {
        "id": "KB0D57Jxg1uD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [

        "def normalize_corpus(corpus, lemmatize=True, tokenize=False):\n",
        "  normalized_corpus = []\n",
        "  for text in corpus:\n",
        "    text = html.unescape(text)\n",
        "    text = expand_contractions(text, CONTRACTION_MAP)\n",
        "    if lemmatize:\n",
        "      text = lemmatize_text(text)\n",
        "    else:\n",
        "      text = text.lower()\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_stopwords(text)\n",
        "    if tokenize:\n",
        "      text = tokenize_text(text)\n",
        "      normalized_corpus.append(text)\n",
        "    else:\n",
        "      normalized_corpus.append(text)\n",
        "  return normalized_corpus"
      ],
      "metadata": {
        "id": "G7O7Lc13jVdx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "def build_feature_matrix(documents, feature_type='frequency'):\n",
        "  feature_type = feature_type.lower().strip()\n",
        "  if feature_type == 'binary':\n",
        "    vectorizer = CountVectorizer(binary=True, min_df=1,ngram_range=(1, 1))\n",
        "  elif feature_type == 'frequency':\n",
        "    vectorizer = CountVectorizer(binary=False, min_df=1,ngram_range=(1, 1))\n",
        "  elif feature_type == 'tfidf':\n",
        "    vectorizer = TfidfVectorizer(min_df=1,ngram_range=(1, 1))\n",
        "  else:\n",
        "    raise Exception(\"Wrong feature type entered. Possible values:'binary', 'frequency', 'tfidf'\")\n",
        "  feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
        "  return vectorizer, feature_matrix"
      ],
      "metadata": {
        "id": "pMGXSEbvlCCK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [

        "from nltk.corpus import gutenberg\n",
        "import nltk\n",
        "from operator import itemgetter\n",
        "\n",

        "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
        "alice = [' '.join(ts) for ts in alice]\n",
        "norm_alice = list(filter(None, normalize_corpus(alice, lemmatize=False)))\n",

        "print(norm_alice[0])\n",
        "\n",

        "def flatten_corpus(corpus):\n",
        "  return ' '.join([document.strip() for document in corpus])\n",
        "\n",

        "def compute_ngrams(sequence, n):\n",
        "  return list(zip(*[sequence[index:] for index in range(n)]))\n",
        "\n",
        "print(compute_ngrams(['He', 'plays','cricket','very','well'],2))\n",
        "\n",
        "print(compute_ngrams(['He', 'plays','cricket','very','well'],3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhkSGFq4kLvY",
        "outputId": "cbf1ec3d-850b-4a3a-989e-9733e8a4eba9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice adventures wonderland lewis carroll 1865\n",
            "[('He', 'plays'), ('plays', 'cricket'), ('cricket', 'very'), ('very', 'well')]\n",
            "[('He', 'plays', 'cricket'), ('plays', 'cricket', 'very'), ('cricket', 'very', 'well')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [

        "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
        "  corpus = flatten_corpus(corpus)\n",
        "  tokens = nltk.word_tokenize(corpus)\n",
        "  ngrams = compute_ngrams(tokens, ngram_val)\n",
        "  ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
        "  sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), key=itemgetter(1), reverse=True)\n",
        "  sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
        "  sorted_ngrams = [(text, freq) for text, freq in sorted_ngrams]\n",
        "  return sorted_ngrams"
      ],
      "metadata": {
        "id": "7PyUINlg6mPi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_top_ngrams(corpus=norm_alice, ngram_val=2, limit=10))\n",
        "print(get_top_ngrams(corpus=norm_alice, ngram_val=3, limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TusXlUaC-hZ8",
        "outputId": "ba522000-05f1-4e61-b1a4-231db434a725"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('said', 'alice'), 123), (('mock', 'turtle'), 56), (('march', 'hare'), 31), (('said', 'king'), 29), (('thought', 'alice'), 26), (('white', 'rabbit'), 22), (('said', 'hatter'), 22), (('said', 'mock'), 20), (('said', 'caterpillar'), 18), (('said', 'gryphon'), 18)]\n",
            "[(('said', 'mock', 'turtle'), 20), (('said', 'march', 'hare'), 10), (('poor', 'little', 'thing'), 6), (('little', 'golden', 'key'), 5), (('certainly', 'said', 'alice'), 5), (('white', 'kid', 'gloves'), 5), (('march', 'hare', 'said'), 5), (('mock', 'turtle', 'said'), 5), (('know', 'said', 'alice'), 4), (('might', 'well', 'say'), 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence=['I', 'love', 'to', 'sing', 'tamil']\n",
        "print([sequence[index:] for index in range(3)])\n",
        "result=list(zip(*[sequence[index:] for index in range(3)]))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "759VP35E29S1",
        "outputId": "fffb19c2-b49e-4b9a-c6fc-a03752599c4b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I', 'love', 'to', 'sing', 'tamil'], ['love', 'to', 'sing', 'tamil'], ['to', 'sing', 'tamil']]\n",
            "[('I', 'love', 'to'), ('love', 'to', 'sing'), ('to', 'sing', 'tamil')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [


        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "finder = BigramCollocationFinder.from_documents([item.split() for item in norm_alice])\n",
        "bigram_measures = BigramAssocMeasures()\n",

        "print('TOP 10 BIGRAMS BASED ON RAW FREQUENCY')\n",
        "print(finder.nbest(bigram_measures.raw_freq, 10))\n",

        "print('TOP 10 BIGRAMS BASED ON PMI')\n",
        "print(finder.nbest(bigram_measures.pmi, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWjq427WK0j9",
        "outputId": "6130ed01-cf84-4f54-d46b-507493f3dff3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP 10 BIGRAMS BASED ON RAW FREQUENCY\n",
            "[('said', 'alice'), ('mock', 'turtle'), ('march', 'hare'), ('said', 'king'), ('thought', 'alice'), ('said', 'hatter'), ('white', 'rabbit'), ('said', 'mock'), ('said', 'caterpillar'), ('said', 'gryphon')]\n",
            "TOP 10 BIGRAMS BASED ON PMI\n",
            "[('abide', 'figures'), ('acceptance', 'elegant'), ('accounting', 'tastes'), ('accustomed', 'usurpation'), ('act', 'crawling'), ('adjourn', 'immediate'), ('adoption', 'energetic'), ('affair', 'trusts'), ('agony', 'terror'), ('alarmed', 'proposal')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [


        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.collocations import TrigramAssocMeasures\n",
        "finder = TrigramCollocationFinder.from_documents([item.split() for item in norm_alice])\n",
        "trigram_measures = TrigramAssocMeasures()\n",

        "print('TOP 10 TRIGRAMS BASED ON RAW FREQUENCY')\n",
        "print(finder.nbest(trigram_measures.raw_freq, 10))\n",

        "print('TOP 10 TRIGRAMS BASED ON PMI')\n",
        "print(finder.nbest(trigram_measures.pmi, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddT1FnVzMQyR",
        "outputId": "7f2543d5-7c3e-460a-e59e-0328ab2f1c2b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP 10 TRIGRAMS BASED ON RAW FREQUENCY\n",
            "[('said', 'mock', 'turtle'), ('said', 'march', 'hare'), ('poor', 'little', 'thing'), ('little', 'golden', 'key'), ('march', 'hare', 'said'), ('mock', 'turtle', 'said'), ('white', 'kid', 'gloves'), ('beau', 'ootiful', 'soo'), ('certainly', 'said', 'alice'), ('might', 'well', 'say')]\n",
            "TOP 10 TRIGRAMS BASED ON PMI\n",
            "[('accustomed', 'usurpation', 'conquest'), ('adjourn', 'immediate', 'adoption'), ('adoption', 'energetic', 'remedies'), ('ancient', 'modern', 'seaography'), ('apple', 'roast', 'turkey'), ('arithmetic', 'ambition', 'distraction'), ('brother', 'latin', 'grammar'), ('canvas', 'bag', 'tied'), ('cherry', 'tart', 'custard'), ('circle', 'exact', 'shape')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [

        "\n",
        "import itertools\n",
        "import nltk\n",
        "from gensim import corpora, models\n",
        "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}'):\n",

        "  all_chunks = []\n",
        "  chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
        "  for sentence in sentences:\n",

        "    tagged_sents = nltk.pos_tag_sents([nltk.word_tokenize(sentence)])\n",

        "    chunks = [chunker.parse(tagged_sent) for tagged_sent in tagged_sents]\n",

        "    wtc_sents = [nltk.chunk.tree2conlltags(chunk) for chunk in chunks]\n",
        "    flattened_chunks = list(itertools.chain.from_iterable( wtc_sent for wtc_sent in wtc_sents))\n",
        "    valid_chunks_tagged = [(status, [wtc for wtc in chunk]) for status, chunk\n",
        "                                        in itertools.groupby(flattened_chunks, lambda x: x[2] != 'O')]\n",

        "    valid_chunks = [' '.join(word.lower() for word, tag, chunk in wtc_group\n",
        "                                            if word.lower() not in stopword_list)\n",
        "                                                for status, wtc_group in valid_chunks_tagged if status]\n",

        "    all_chunks.append(valid_chunks)\n",
        "  return all_chunks\n",
        "\n",
        "sentences = parse_document(toy_text)\n",
        "valid_chunks = get_chunks(sentences)\n",

        "print(valid_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRNuyZfT--ki",
        "outputId": "74c19dcc-0d69-4d92-a4ce-158ab360a4f5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['elephants', 'large mammals', 'family elephantidae', 'order proboscidea'], ['species', 'african elephant', 'asian elephant'], ['elephants', 'sub-saharan africa', 'south asia', 'southeast asia'], ['male african elephants', 'extant terrestrial animals'], ['elephants', 'long trunk', 'many purposes', 'breathing', 'water', 'grasping objects'], ['incisors', 'tusks', 'weapons', 'tools', 'objects', 'digging'], ['elephants', 'large ear flaps', 'body temperature'], ['pillar-like legs', 'great weight'], ['african elephants', 'ears', 'backs', 'asian elephants', 'ears', 'convex', 'level backs']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "def low_rank_svd(matrix, singular_count=2):\n",
        "  u, s, vt = svds(matrix, k=singular_count)\n",
        "  return u, s, vt"
      ],
      "metadata": {
        "id": "pnjc-AmmVmP2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [

        "import numpy as np\n",
        "toy_text = \"\"\"\n",
        "Elephants are large mammals of the family Elephantidae\n",
        "and the order Proboscidea. Two species are traditionally recognised,\n",
        "the African elephant and the Asian elephant. Elephants are scattered\n",
        "throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male\n",
        "African elephants are the largest extant terrestrial animals. All\n",
        "elephants have a long trunk used for many purposes,\n",
        "particularly breathing, lifting water and grasping objects. Their\n",
        "incisors grow into tusks, which can serve as weapons and as tools\n",
        "for moving objects and digging. Elephants' large ear flaps help\n",
        "to control their body temperature. Their pillar-like legs can\n",
        "carry their great weight. African elephants have larger ears\n",
        "and concave backs while Asian elephants have smaller ears\n",
        "and convex or level backs.\n",
        "\"\"\"\n",
        "\n",

        "num_sentences = 3\n",
        "num_topics = 3\n",
        "\n",

        "sentences = parse_document(toy_text)\n",
        "norm_sentences = normalize_corpus(sentences,lemmatize=True)\n",
        "print('Total Sentences in Document:', len(norm_sentences))\n",
        "\n",
        "def lsa_text_summarizer(documents, num_sentences=2, num_topics=2, feature_type='frequency', sv_threshold=0.5):\n",
        "  vec, dt_matrix = build_feature_matrix(documents, feature_type=feature_type)\n",
        "  td_matrix = dt_matrix.transpose()\n",
        "  td_matrix = td_matrix.multiply(td_matrix > 0)\n",
        "  u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)\n",
        "  min_sigma_value = max(s) * sv_threshold\n",
        "  s[s < min_sigma_value] = 0\n",
        "  salience_scores = np.sqrt(np.dot(np.square(s), np.square(vt)))\n",
        "  top_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\n",
        "  top_sentence_indices.sort()\n",
        "  for index in top_sentence_indices:\n",
        "    print(sentences[index])\n",
        "\n",
        "print(lsa_text_summarizer(sentences,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwLk1Qr0azNu",
        "outputId": "23f82586-cfcb-4010-fa14-80190a7ab38d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sentences in Document: 9\n",
            "All elephants have a long trunk used for many purposes, particularly breathing, lifting water and grasping objects.\n",
            "Their incisors grow into tusks, which can serve as weapons and as tools for moving objects and digging.\n",
            "African elephants have larger ears and concave backs while Asian elephants have smaller ears and convex or level backs.\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}
