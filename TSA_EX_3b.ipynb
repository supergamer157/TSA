{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7CjeXvIorHG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from pprint import pprint\n",
        "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for $199\",\n",
        "\"@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"] def tokenize_text(text):\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]  return word_tokens\n",
        "token_list = [tokenize_text(text) for text in corpus]\n",
        "pprint(token_list)\n",
        "def remove_characters_after_tokenization(tokens):\n",
        "pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
        "filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens]) return filtered_tokens\n",
        "filtered_list_1 = [filter(None,[remove_characters_after_tokenization(tokens)  for tokens in sentence_tokens])\n",
        "for sentence_tokens in token_list]\n",
        "print (filtered_list_1)\n",
        "\n",
        "\n",
        "print (corpus[0].lower())\n",
        "print (corpus[0].upper())\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "filtered_tokens = [token for token in tokens if token not in stopword_list] return filtered_tokens\n",
        "cleaned_corpus_tokens = [tokenize_text(text)\n",
        "for text in corpus]\n",
        "filtered_list_3 = [[remove_stopwords(tokens)\n",
        "for tokens in sentence_tokens]\n",
        "for sentence_tokens in corpus]\n",
        "print (filtered_list_3)\n",
        "\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print (ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')) print (ps.stem('lying'))\n",
        "print (ps.stem('strange'))\n"
      ]
    }
  ]
}